{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install verdict\n",
    "!uv pip install verdict --system\n",
    "\n",
    "# This notebook has been run ahead of time, so you can inspect outputs without making\n",
    "# any API calls. You can set your API key if you want to run the examples yourself.\n",
    "# %env OPENAI_API_KEY=*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExpertQA Hallucination Detection\n",
    "#### See https://verdict.haizelabs.com/docs for a walkthrough of this example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " <span style=\"font-weight: bold\"> Ground Truth </span> <span style=\"font-weight: bold\"> Prediction </span> <span style=\"font-weight: bold\">  </span> <span style=\"font-weight: bold\">        Acc.        </span> <span style=\"font-weight: bold\">     Cohen (κ)      </span> <span style=\"font-weight: bold\">    Kendall (τ)    </span> <span style=\"font-weight: bold\">    Spearman (ρ)    </span> \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "         label   pred_label            78.88%                0.02                0.03                 0.03         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPrediction\u001b[0m\u001b[1m \u001b[0m \u001b[1m  \u001b[0m \u001b[1m \u001b[0m\u001b[1m       Acc.       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m    Cohen (κ)     \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Kendall (τ)   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Spearman (ρ)   \u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "         label   pred_label            78.88%                0.02                0.03                 0.03         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from verdict import Pipeline, Layer\n",
    "from verdict.common.judge import CategoricalJudgeUnit\n",
    "from verdict.scale import DiscreteScale\n",
    "from verdict.transform import MaxPoolUnit\n",
    "from verdict.dataset import DatasetWrapper\n",
    "\n",
    "# We'll disable Verdict rate-limiting for this example. By default, Verdict follows the OpenAI Tier 1 rate limits for `gpt-4o-mini`.\n",
    "from verdict.util import ratelimit\n",
    "ratelimit.disable()\n",
    "\n",
    "#   1. A CategoricalJudgeUnit first decides if a hallucination is present...\n",
    "#   2. Followed by a CategoricalJudgeUnit that verifies the initial judge's explanation and decision.\n",
    "\n",
    "judge_then_verify = CategoricalJudgeUnit(name='Judge', categories=DiscreteScale(['yes', 'no']), explanation=True).prompt(\"\"\"\n",
    "    Determine whether the provided claim is consistent with the corresponding document. Consistency in this context implies that\n",
    "    all information presented in the claim is substantiated by the document. If not, it should be considered inconsistent.\n",
    "    Document: {source.doc}\n",
    "    Claim: {source.claim}\n",
    "    Please assess the claim’s consistency with the document by responding with either \"yes\" or \"no\".\n",
    "    Answer:\n",
    "\"\"\").via(policy_or_name='gpt-4o', retries=3, temperature=0.7) \\\n",
    "\\\n",
    ">> CategoricalJudgeUnit(name='Verifier', categories=DiscreteScale(['yes', 'no'])).prompt(\"\"\"\n",
    "    Check if the given answer correctly reflects whether the claim is consistent with the corresponding document. Consistency in this context implies that\n",
    "    all information presented in the claim is substantiated by the document. If not, it should be considered inconsistent. \"yes\" means the claim is consistent with the document, and \"no\" means the claim is not consistent with the document.\n",
    "    \n",
    "    Document: {source.doc}\n",
    "    Claim: {source.claim}\n",
    "    Answer: {previous.choice}\n",
    "    Answer Justification: {previous.explanation}\n",
    "    \n",
    "    If you think the answer is correct, return the answer as is. If it's incorrect, return the opposite (if answer = \"yes\", return \"no\", and if answer = \"no\", return \"yes\").\n",
    "    \"\"\").via(policy_or_name='gpt-4o', retries=3, temperature=0.0)\n",
    "\n",
    "# ...and then aggregate the results of the three judges+verifiers with a `MaxPoolUnit`\n",
    "pipeline = Pipeline() \\\n",
    "    >> Layer(judge_then_verify, repeat=3) \\\n",
    "    >> MaxPoolUnit()\n",
    "\n",
    "# Load the ExpertQA dataset (https://arxiv.org/pdf/2309.07852)\n",
    "dataset = DatasetWrapper.from_hf(\n",
    "    load_dataset('lytang/LLM-AggreFact').filter(\n",
    "        lambda row: row['dataset'] == 'ExpertQA'\n",
    "    ),\n",
    "    columns=['claim', 'doc', 'label'] # we can also specify a custom input_fn to map each row into a Schema\n",
    ")\n",
    "\n",
    "# all responses from intermediate Units are available as columns in response_df!\n",
    "response_df, _ = pipeline.run_from_dataset(dataset['test'], max_workers=1024)\n",
    "response_df['pred_label'] = response_df['Pipeline_root.block.block.unit[Map MaxPool]_choice'] == 'yes'\n",
    "\n",
    "from verdict.util.experiment import display_stats, ExperimentConfig\n",
    "display_stats(response_df, ExperimentConfig(ground_truth_cols=['label'], prediction_cols=['pred_label']));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

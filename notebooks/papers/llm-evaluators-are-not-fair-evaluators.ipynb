{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install verdict\n",
    "!uv pip install verdict --system\n",
    "\n",
    "# data\n",
    "!wget https://raw.githubusercontent.com/i-Eval/FairEval/refs/heads/main/question.jsonl --no-clobber\n",
    "!wget https://raw.githubusercontent.com/i-Eval/FairEval/refs/heads/main/review/review_gpt35_vicuna-13b_human.txt --no-clobber\n",
    "!wget https://raw.githubusercontent.com/i-Eval/FairEval/refs/heads/main/answer/answer_gpt35.jsonl --no-clobber\n",
    "!wget https://raw.githubusercontent.com/i-Eval/FairEval/refs/heads/main/answer/answer_vicuna-13b.jsonl --no-clobber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde1698-458f-40cd-8c2d-365b7585bbb0",
   "metadata": {},
   "source": [
    "> [**Large Language Model Evaluators are not Fair Evaluators**](https://arxiv.org/abs/2305.17926)  \n",
    "> Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui  \n",
    "> ACL 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab184ff0-b695-43eb-bfc3-4596c02c89f3",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb15aad6-b27e-4411-aaa7-466c6fd7ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from verdict.dataset import DatasetWrapper\n",
    "from verdict.schema import Schema\n",
    "\n",
    "df = pd.read_json(\"./question.jsonl\", lines=True).join(\n",
    "    pd.read_csv(\"./review_gpt35_vicuna-13b_human.txt\", names=[\"judgement\"])\n",
    ").set_index(\"question_id\").rename(columns={\"text\": \"question\"}).join(\n",
    "    pd.read_json(\"./answer_gpt35.jsonl\", lines=True).set_index(\"question_id\")[[\"text\"]].rename(columns={\"text\": \"answer_gpt35\"})\n",
    ").join(\n",
    "    pd.read_json(\"./answer_vicuna-13b.jsonl\", lines=True).set_index(\"question_id\")[[\"text\"]].rename(columns={\"text\": \"answer_vicuna-13b\"})\n",
    ")\n",
    "\n",
    "dataset = DatasetWrapper.from_pandas(\n",
    "    df,\n",
    "    lambda row: Schema.of(\n",
    "        question=row[\"question\"],\n",
    "        answer_assistant_1=row[\"answer_gpt35\"],\n",
    "        answer_assistant_2=row[\"answer_vicuna-13b\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a15aa5-eb84-4be8-bc22-cce4481f9467",
   "metadata": {},
   "source": [
    "## Custom Judge Unit + Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047c5f91-0784-4b07-aac8-738fe7033aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verdict import Pipeline, Unit\n",
    "from verdict.prompt import Prompt\n",
    "from verdict.schema import Schema\n",
    "\n",
    "class PairwiseEvidenceCalibrationJudgeUnit(Unit):\n",
    "    class InputSchema(Schema):\n",
    "        question: str\n",
    "        answer_assistant_1: str\n",
    "        answer_assistant_2: str\n",
    "\n",
    "    class ResponseSchema(Schema):\n",
    "        evaluation_evidence: str # Evidence Calibration (Section 3.1)\n",
    "        score_assistant_1: float\n",
    "        score_assistant_2: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07702989-3a9c-4097-9300-5004f1604fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "    [Question]\n",
    "    {source.question}\n",
    "    [The Start of Assistant 1's response]\n",
    "    {source.answer_assistant_1}\n",
    "    [The End of Assistant 1's response]\n",
    "    [The Start of Assistant 2's response]\n",
    "    {source.answer_assistant_2}\n",
    "    [The End of Assistant 2's response]\n",
    "\n",
    "    @system\n",
    "    We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "    Please rate the helpfulness, relevance, accuracy, and level of detail of their responses.\n",
    "    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n",
    "    Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\n",
    "    Then, output scores for Assistant 1 and 2, respectively.\n",
    "\"\"\"\n",
    "\n",
    "pipeline = Pipeline(\"Pairwise\") \\\n",
    "    >> PairwiseEvidenceCalibrationJudgeUnit().prompt(PROMPT).via(\"gpt-4\", retries=3)\n",
    "\n",
    "df, _ = pipeline.run_from_dataset(dataset['all'], max_workers=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884caa26-7dfd-4427-b0ce-77be2646ad72",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212dc23a-7762-4e72-9d05-690196b1574b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " <span style=\"font-weight: bold\"> Ground Truth </span> <span style=\"font-weight: bold\"> Prediction </span> <span style=\"font-weight: bold\">  </span> <span style=\"font-weight: bold\">        Acc.        </span> <span style=\"font-weight: bold\">     Cohen (κ)      </span> <span style=\"font-weight: bold\">    Kendall (τ)    </span> <span style=\"font-weight: bold\">    Spearman (ρ)    </span> \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "     judgement   _judgement            48.75%                0.21                0.34                 0.38         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPrediction\u001b[0m\u001b[1m \u001b[0m \u001b[1m  \u001b[0m \u001b[1m \u001b[0m\u001b[1m       Acc.       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m    Cohen (κ)     \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Kendall (τ)   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Spearman (ρ)   \u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "     judgement   _judgement            48.75%                0.21                0.34                 0.38         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from verdict.util.experiment import ExperimentConfig, display_stats\n",
    "\n",
    "import numpy as np\n",
    "def add_judgement_column(df, score_assistant_1_col, score_assistant_2_col):\n",
    "    df['_judgement'] = np.select([\n",
    "        df[score_assistant_1_col] > df[score_assistant_2_col],\n",
    "        df[score_assistant_1_col] == df[score_assistant_2_col]\n",
    "    ], [\n",
    "        'CHATGPT',\n",
    "        'TIE'\n",
    "    ], default='VICUNA13B')\n",
    "    return df\n",
    "\n",
    "df = add_judgement_column(df, 'Pairwise_root.block.unit[Unit]_score_assistant_1', 'Pairwise_root.block.unit[Unit]_score_assistant_2')\n",
    "display_stats(df, ExperimentConfig(\n",
    "    ground_truth_cols=['judgement'],\n",
    "    prediction_cols=['_judgement']\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ababb-d602-49d5-aeac-be7e7cc0fa13",
   "metadata": {},
   "source": [
    "### MEC (Multiple Evidence Calibration, Section 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563592c8-facb-4de0-bfa8-177b8e89fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verdict import Layer\n",
    "from verdict.transform import MeanPoolUnit\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    >> Layer(\n",
    "        PairwiseEvidenceCalibrationJudgeUnit().prompt(PROMPT).via(\"gpt-4\", retries=3)\n",
    "    , 3) \\\n",
    "    >> MeanPoolUnit([\"score_assistant_1\", \"score_assistant_2\"])\n",
    "\n",
    "df, _ = pipeline.run_from_dataset(dataset['all'], max_workers=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a2d7f65-ffa3-4254-b22a-a59a43ba4eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " <span style=\"font-weight: bold\"> Ground Truth </span> <span style=\"font-weight: bold\"> Prediction </span> <span style=\"font-weight: bold\">  </span> <span style=\"font-weight: bold\">        Acc.        </span> <span style=\"font-weight: bold\">     Cohen (κ)      </span> <span style=\"font-weight: bold\">    Kendall (τ)    </span> <span style=\"font-weight: bold\">    Spearman (ρ)    </span> \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "     judgement   _judgement            62.50%                0.39                0.50                 0.54         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPrediction\u001b[0m\u001b[1m \u001b[0m \u001b[1m  \u001b[0m \u001b[1m \u001b[0m\u001b[1m       Acc.       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m    Cohen (κ)     \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Kendall (τ)   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Spearman (ρ)   \u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "     judgement   _judgement            62.50%                0.39                0.50                 0.54         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = add_judgement_column(df, 'Pipeline_root.block.block.unit[Map MeanPool]_score_assistant_1', 'Pipeline_root.block.block.unit[Map MeanPool]_score_assistant_2')\n",
    "display_stats(df, ExperimentConfig(\n",
    "    ground_truth_cols=['judgement'],\n",
    "    prediction_cols=['_judgement']\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7f206-40a0-4581-b2fb-bae7e5cc1e78",
   "metadata": {},
   "source": [
    "### MEC + BPC (Balanced Position Calibration, Section 3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074c9d09-f3e3-4bab-8504-608b7127e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verdict.transform import MapUnit\n",
    "\n",
    "PROMPT_REVERSED = \"\"\"\n",
    "    [Question]\n",
    "    {source.question}\n",
    "    [The Start of Assistant 1's response]\n",
    "    {source.answer_assistant_2}\n",
    "    [The End of Assistant 1's response]\n",
    "    [The Start of Assistant 2's response]\n",
    "    {source.answer_assistant_1}\n",
    "    [The End of Assistant 2's response]\n",
    "\n",
    "    @system\n",
    "    We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.\n",
    "    Please rate the helpfulness, relevance, accuracy, and level of detail of their responses.\n",
    "    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n",
    "    Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\n",
    "    Then, output scores for Assistant 1 and 2, respectively.\n",
    "\"\"\"\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    >> Layer([\n",
    "        PairwiseEvidenceCalibrationJudgeUnit().prompt(PROMPT),\n",
    "        PairwiseEvidenceCalibrationJudgeUnit().prompt(PROMPT_REVERSED).propagate(lambda unit, previous, input, output: Schema.of(\n",
    "            score_assistant_1=output.score_assistant_2,\n",
    "            score_assistant_2=output.score_assistant_1\n",
    "        ))\n",
    "    ], 3).via(\"gpt-4\", retries=3) \\\n",
    "    >> MeanPoolUnit([\"score_assistant_1\", \"score_assistant_2\"])\n",
    "\n",
    "df, _ = pipeline.run_from_dataset(dataset['all'], max_workers=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b8275b-2fad-4010-8710-3b6425b6d6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " <span style=\"font-weight: bold\"> Ground Truth </span> <span style=\"font-weight: bold\"> Prediction </span> <span style=\"font-weight: bold\">  </span> <span style=\"font-weight: bold\">        Acc.        </span> <span style=\"font-weight: bold\">     Cohen (κ)      </span> <span style=\"font-weight: bold\">    Kendall (τ)    </span> <span style=\"font-weight: bold\">    Spearman (ρ)    </span> \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "     judgement   _judgement            36.25%                0.01                0.05                 0.05         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPrediction\u001b[0m\u001b[1m \u001b[0m \u001b[1m  \u001b[0m \u001b[1m \u001b[0m\u001b[1m       Acc.       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m    Cohen (κ)     \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Kendall (τ)   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   Spearman (ρ)   \u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "     judgement   _judgement            36.25%                0.01                0.05                 0.05         \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = add_judgement_column(df, 'Pipeline_root.block.block.unit[Map MeanPool]_score_assistant_1', 'Pipeline_root.block.block.unit[Map MeanPool]_score_assistant_2')\n",
    "display_stats(df, ExperimentConfig(\n",
    "    ground_truth_cols=['judgement'],\n",
    "    prediction_cols=['_judgement']\n",
    "));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
